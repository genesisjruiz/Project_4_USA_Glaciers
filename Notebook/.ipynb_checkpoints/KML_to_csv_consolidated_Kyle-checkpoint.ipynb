{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379eb5c2-a1cf-4faa-8c04-f39c0bb71063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload kml file\n",
    "from pykml import parser\n",
    "import pandas as pd\n",
    "\n",
    "with open('../Resources/glims_polygons.kml', 'r', encoding=\"utf-8\") as f:\n",
    "   root = parser.parse(f).getroot()\n",
    "#Put kml to dataframe  \n",
    "places = []\n",
    "for place in root.Document.Folder.Placemark:\n",
    "    data = {item.get(\"name\"): item.text for item in\n",
    "            place.ExtendedData.SchemaData.SimpleData}\n",
    "    coords = place.Polygon.outerBoundaryIs.LinearRing.coordinates.text.strip()\n",
    "    data[\"Coordinates\"] = coords\n",
    "    places.append(data)\n",
    "df = pd.DataFrame(places)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8212b8-3e17-4f90-b0bf-a11791bd27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f702506-644c-4e79-ab6b-1cdf5e0029ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a7215de-32cb-4541-a608-0213a1646f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 69572\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = df[['line_type','anlys_id','area','glac_id','anlys_time','db_area','min_elev','mean_elev','max_elev','primeclass','Coordinates','src_date','rec_status','glac_name','glac_stat',\n",
    "'gone_date','gone_dt_e','subm_id','release_dt','proc_desc','rc_id','geog_area','conn_lvl','surge_type','term_type','gtng_o1reg','gtng_o2reg',\n",
    "'rgi_gl_typ','loc_unc_x','glob_unc_y']]\n",
    "\n",
    "# Counting initial number of rows to make sure we don't somehow lose any.\n",
    "row_count = cleaned_df.shape[0]\n",
    "print(f\"Number of rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e75a674-4e83-4d51-a9a6-7558583395f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0daaaf4f-9148-4673-af24-740645bb0c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dictionary to store unique values DataFrames\n",
    "unique_dfs = {}\n",
    "\n",
    "# Iterate over each column, get unique values, and create DataFrame\n",
    "for column in cleaned_df.columns:\n",
    "    unique_values = cleaned_df[column].unique()\n",
    "    unique_dfs[column] = pd.DataFrame(unique_values, columns=[f'unique_{column}'])\n",
    "\n",
    "# for column, unique_df in unique_dfs.items():\n",
    "#     print(f\"Unique values in {column} as DataFrame:\")\n",
    "#     print(unique_df)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e8438b-76cc-4ee1-b55e-cc31c5399280",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_glac_name = 'glac_name'\n",
    "glac_name_df = unique_dfs.get(unique_glac_name)\n",
    "\n",
    "glac_name_df.to_csv('../Resources/glac_name_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7238f1be-d705-4fe9-aa0a-b39a432e73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE ABOVE FOR DEFINITIONS. ELIMINATING PRIMECLASS, REC_STATUS, SUBM_ID, GONE_DT_E, PROC_DESC, RC_ID, GEOG_AREA, CONN_LVL, SURGE_TYPE, TERM_TYPE, GTNG_O1REG, GTNG_O2REG, RGI_GL_TYP,\n",
    "# LOC_UNC_X, GLOB_UNC_Y\n",
    "\n",
    "# SOME ARE NOTES, SOME ARE DEFINITIONS RELEVANT TO SOMEONE. LIKELY NOT TO US. REC_STATUS HAD ONE VALUE; 'OKAY'.\n",
    "\n",
    "cleaned_df_2 = df[['line_type','anlys_id','area','glac_id','anlys_time','db_area','min_elev','mean_elev','max_elev','Coordinates','src_date','glac_name','glac_stat',\n",
    "'gone_date','release_dt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "271c99dd-b48c-4fbc-bf77-606733899256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a df copy\n",
    "cleaned_df_3 = cleaned_df_2.copy()\n",
    "\n",
    "# glac_stat has two unique values. replaced exists with e and gone with g\n",
    "cleaned_df_3['glac_stat'] = cleaned_df_3['glac_stat'].replace({'gone': 'g', 'exists': 'e'})\n",
    "\n",
    "# line_type has three unique values. Replaced debris_cov with dc, glac_bound with gb, intrnl_rock with ir\n",
    "cleaned_df_3['line_type'] = cleaned_df_3['line_type'].replace({'debris_cov': 'dc', 'glac_bound': 'gb', 'intrnl_rock': 'ir'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa186c8-6d32-4b24-8a0c-ace67601cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changed the times which were strings to times that are datetime objects. Took away the hour and rounded off to a day, which\n",
    "# wasn't intentional, but we don't need hourly resolution for a measurement of an object that changes very slowly\n",
    "# and that we are only seeing measurements for a few times per year at most\n",
    "\n",
    "cleaned_df_3['src_date'] = pd.to_datetime(cleaned_df_3['src_date'])\n",
    "cleaned_df_3['anlys_time'] = pd.to_datetime(cleaned_df_3['anlys_time'])\n",
    "cleaned_df_3['release_dt'] = pd.to_datetime(cleaned_df_3['release_dt'])\n",
    "\n",
    "\n",
    "# Converted the times to unix. Unix times before 1/1/1970 are negative numbers, descending from 0 below 1/1/1970, so they'll still work and be\n",
    "# convertible and comparable.\n",
    "# Of note, minimum recording is 12/13/1901, so if there were dates before that, this would not be feasible.\n",
    "\n",
    "cleaned_df_3['src_date'] = cleaned_df_3['src_date'].apply(lambda x: int(x.timestamp()))\n",
    "cleaned_df_3['anlys_time'] = cleaned_df_3['anlys_time'].apply(lambda x: int(x.timestamp()))\n",
    "cleaned_df_3['release_dt'] = cleaned_df_3['release_dt'].apply(lambda x: int(x.timestamp()))\n",
    "\n",
    "# format for gone_date. the object wasn't recognized as a time like the other was, so I explicitly defined the format.\n",
    "gone_date_raw_format = '%Y-%m-%d'\n",
    "\n",
    "# Converting gone_date to datetime.\n",
    "cleaned_df_3['gone_date'] = pd.to_datetime(cleaned_df_3['gone_date'], format=gone_date_raw_format, errors='coerce')\n",
    "\n",
    "# Converting gone_date to unix. If the glacier still exists, it's a NaT, so replacing it with e for exists.\n",
    "nat_replace_if_exists = 'e'\n",
    "cleaned_df_3['gone_date'] = cleaned_df_3['gone_date'].apply(\n",
    "    lambda x: int(x.timestamp()) if pd.notna(x) else nat_replace_if_exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aef7f78-19d0-44bc-818a-502b20a2fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying data frame\n",
    "\n",
    "cleaned_df_4 = cleaned_df_3.copy()\n",
    "\n",
    "# Converting columns to float. errors='coerce' won't effect 0 values, but any non-number will be converted to nan. Will print between this conversion and\n",
    "# the formatting just to make sure.\n",
    "\n",
    "cleaned_df_4['area'] = pd.to_numeric(cleaned_df_4['area'], errors='coerce')\n",
    "cleaned_df_4['db_area'] = pd.to_numeric(cleaned_df_4['db_area'], errors='coerce')\n",
    "\n",
    "nat_replace = 'x'\n",
    "\n",
    "# nan_rows # no nan rows for either area or db_area, so we're good, per the below check.\n",
    "# nan_rows = cleaned_df_4[cleaned_df_4['db_area'].isna()]\n",
    "\n",
    "# converting to float with the trailing digits removed. The number is the number of the significant digits we round to.\n",
    "\n",
    "for column in ['area', 'db_area']:\n",
    "    cleaned_df_4[column] = cleaned_df_4[column].apply(lambda x: '{:.15g}'.format(x) if pd.notnull(x) else nat_replace)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "258c2ba6-3dd8-4173-9e77-a70aa36375d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the analysis ID column for cleaned_df_5. We LIKELY don't need it.\n",
    "\n",
    "cleaned_df_5 = cleaned_df_4.drop(columns=['anlys_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb4d31ea-8e4a-4e7a-856f-75735f05218f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glac_name</th>\n",
       "      <th>unique_glac_id_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>Yalik Glacier</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>Yanert Glacier</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>Yawning Glacier</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>Yentna Glacier</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>Zigzag Glacier</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1655 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            glac_name  unique_glac_id_count\n",
       "0                   1                     2\n",
       "1                  10                     2\n",
       "2                 100                     2\n",
       "3                1000                     1\n",
       "4                1001                     1\n",
       "...               ...                   ...\n",
       "1650    Yalik Glacier                     4\n",
       "1651   Yanert Glacier                     1\n",
       "1652  Yawning Glacier                     2\n",
       "1653   Yentna Glacier                     1\n",
       "1654   Zigzag Glacier                     3\n",
       "\n",
       "[1655 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # This is to view how many unique glac_id exist per glac_name.\n",
    "\n",
    "# cleaned_df_6 = cleaned_df_5.copy()\n",
    "\n",
    "# # Group by glac_name and count unique glac_id values\n",
    "# glac_id_counts = cleaned_df_6.groupby('glac_name')['glac_id'].nunique().reset_index()\n",
    "\n",
    "# # Rename columns for clarity\n",
    "# glac_id_counts.columns = ['glac_name', 'unique_glac_id_count']\n",
    "\n",
    "# glac_id_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "653e2f53-3e67-434c-b5de-842ebcb9d17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_type</th>\n",
       "      <th>area</th>\n",
       "      <th>glac_id</th>\n",
       "      <th>anlys_time</th>\n",
       "      <th>db_area</th>\n",
       "      <th>min_elev</th>\n",
       "      <th>mean_elev</th>\n",
       "      <th>max_elev</th>\n",
       "      <th>Coordinates</th>\n",
       "      <th>src_date</th>\n",
       "      <th>glac_name</th>\n",
       "      <th>glac_stat</th>\n",
       "      <th>gone_date</th>\n",
       "      <th>release_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48062</th>\n",
       "      <td>gb</td>\n",
       "      <td>0.061</td>\n",
       "      <td>G246234E48788N</td>\n",
       "      <td>1154390400</td>\n",
       "      <td>0.061358</td>\n",
       "      <td>2213</td>\n",
       "      <td>0</td>\n",
       "      <td>2419</td>\n",
       "      <td>-113.769141,48.789585,0 -113.769657,48.789447,...</td>\n",
       "      <td>-104544000</td>\n",
       "      <td>North Swiftcurrent Glacier</td>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "      <td>1456477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66417</th>\n",
       "      <td>gb</td>\n",
       "      <td>0.053389</td>\n",
       "      <td>G246234E48788N</td>\n",
       "      <td>1676505600</td>\n",
       "      <td>0.053391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-113.768572,48.789314,0 -113.768584,48.789317,...</td>\n",
       "      <td>1440201600</td>\n",
       "      <td>North Swiftcurrent Glacier</td>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "      <td>1684195200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      line_type      area         glac_id  anlys_time   db_area min_elev  \\\n",
       "48062        gb     0.061  G246234E48788N  1154390400  0.061358     2213   \n",
       "66417        gb  0.053389  G246234E48788N  1676505600  0.053391        0   \n",
       "\n",
       "      mean_elev max_elev                                        Coordinates  \\\n",
       "48062         0     2419  -113.769141,48.789585,0 -113.769657,48.789447,...   \n",
       "66417         0        0  -113.768572,48.789314,0 -113.768584,48.789317,...   \n",
       "\n",
       "         src_date                   glac_name glac_stat gone_date  release_dt  \n",
       "48062  -104544000  North Swiftcurrent Glacier         e         e  1456477200  \n",
       "66417  1440201600  North Swiftcurrent Glacier         e         e  1684195200  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_value = 'North Swiftcurrent Glacier'\n",
    "\n",
    "filtered_df_1 = cleaned_df_5[cleaned_df_5['glac_name'] == filter_value]\n",
    "\n",
    "filtered_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d965cdb8-21f2-4f92-959f-9b1818b52986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 69572\n"
     ]
    }
   ],
   "source": [
    "# We still have the same number of rows. Check the cell above that lists row count for cleaned_df\n",
    "row_count = cleaned_df_5.shape[0]\n",
    "print(f\"Number of rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39e83cb7-c238-4894-bebb-50f7d400c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_csv('../Resources/cleaned_Canada_Glaciers.csv', index=False) #342 megs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fca9bec-8623-4958-8a71-5c5734da0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_2.to_csv('../Resources/cleaned_df_2.csv', index=False) #313 megs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8dde813-751a-440a-833c-a0b7811a03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_3.to_csv('../Resources/cleaned_df_3.csv', index=False) #310 megs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9c4fdb7-5b61-4cf7-8669-acba9acaa092",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_4.to_csv('../Resources/cleaned_df_4.csv', index=False) #309 megs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7759249-6220-404e-b5b4-7c4be0218658",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_5.to_csv('../Resources/cleaned_df_5.csv', index=False) #309 megs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
