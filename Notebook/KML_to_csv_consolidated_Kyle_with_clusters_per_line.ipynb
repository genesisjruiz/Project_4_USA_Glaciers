{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379eb5c2-a1cf-4faa-8c04-f39c0bb71063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload kml file\n",
    "from pykml import parser\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "with open('../Resources/glims_polygons.kml', 'r', encoding=\"utf-8\") as f:\n",
    "   root = parser.parse(f).getroot()\n",
    "#Put kml to dataframe  \n",
    "places = []\n",
    "for place in root.Document.Folder.Placemark:\n",
    "    data = {item.get(\"name\"): item.text for item in\n",
    "            place.ExtendedData.SchemaData.SimpleData}\n",
    "    coords = place.Polygon.outerBoundaryIs.LinearRing.coordinates.text.strip()\n",
    "    data[\"Coordinates\"] = coords\n",
    "    places.append(data)\n",
    "df = pd.DataFrame(places)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8212b8-3e17-4f90-b0bf-a11791bd27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f702506-644c-4e79-ab6b-1cdf5e0029ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7215de-32cb-4541-a608-0213a1646f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 69572\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = df[['line_type','anlys_id','area','glac_id','anlys_time','db_area','min_elev','mean_elev','max_elev','primeclass','Coordinates','src_date','rec_status','glac_name','glac_stat',\n",
    "'gone_date','gone_dt_e','subm_id','release_dt','proc_desc','rc_id','geog_area','conn_lvl','surge_type','term_type','gtng_o1reg','gtng_o2reg',\n",
    "'rgi_gl_typ','loc_unc_x','glob_unc_y']]\n",
    "\n",
    "# Counting initial number of rows to make sure we don't somehow lose any.\n",
    "row_count = cleaned_df.shape[0]\n",
    "print(f\"Number of rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e75a674-4e83-4d51-a9a6-7558583395f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0daaaf4f-9148-4673-af24-740645bb0c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dictionary to store unique values DataFrames\n",
    "unique_dfs = {}\n",
    "\n",
    "# Iterate over each column, get unique values, and create DataFrame\n",
    "for column in cleaned_df.columns:\n",
    "    unique_values = cleaned_df[column].unique()\n",
    "    unique_dfs[column] = pd.DataFrame(unique_values, columns=[f'unique_{column}'])\n",
    "\n",
    "# for column, unique_df in unique_dfs.items():\n",
    "#     print(f\"Unique values in {column} as DataFrame:\")\n",
    "#     print(unique_df)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e8438b-76cc-4ee1-b55e-cc31c5399280",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_glac_name = 'glac_name'\n",
    "glac_name_df = unique_dfs.get(unique_glac_name)\n",
    "\n",
    "glac_name_df.to_csv('../Resources/glac_name_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7238f1be-d705-4fe9-aa0a-b39a432e73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE ABOVE FOR DEFINITIONS. ELIMINATING PRIMECLASS, REC_STATUS, SUBM_ID, GONE_DT_E, PROC_DESC, RC_ID, GEOG_AREA, CONN_LVL, SURGE_TYPE, TERM_TYPE, GTNG_O1REG, GTNG_O2REG, RGI_GL_TYP,\n",
    "# LOC_UNC_X, GLOB_UNC_Y\n",
    "\n",
    "# SOME ARE NOTES, SOME ARE DEFINITIONS RELEVANT TO SOMEONE. LIKELY NOT TO US. REC_STATUS HAD ONE VALUE; 'OKAY'.\n",
    "\n",
    "cleaned_df_2 = df[['line_type','anlys_id','area','glac_id','anlys_time','db_area','min_elev','mean_elev','max_elev','Coordinates','src_date','glac_name','glac_stat',\n",
    "'gone_date','release_dt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "271c99dd-b48c-4fbc-bf77-606733899256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a df copy\n",
    "cleaned_df_3 = cleaned_df_2.copy()\n",
    "\n",
    "# glac_stat has two unique values. replaced exists with e and gone with g\n",
    "cleaned_df_3['glac_stat'] = cleaned_df_3['glac_stat'].replace({'gone': 'g', 'exists': 'e'})\n",
    "\n",
    "# line_type has three unique values. Replaced debris_cov with dc, glac_bound with gb, intrnl_rock with ir\n",
    "cleaned_df_3['line_type'] = cleaned_df_3['line_type'].replace({'debris_cov': 'dc', 'glac_bound': 'gb', 'intrnl_rock': 'ir'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa186c8-6d32-4b24-8a0c-ace67601cdaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 10) (3022640951.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    Converted the times to unix. Unix times before 1/1/1970 are negative numbers, descending from 0 below 1/1/1970, so they'll still work and be\u001b[0m\n\u001b[1;37m                                                                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 10)\n"
     ]
    }
   ],
   "source": [
    "# Changed the times which were strings to times that are datetime objects. Took away the hour and rounded off to a day, which\n",
    "# wasn't intentional, but we don't need hourly resolution for a measurement of an object that changes very slowly\n",
    "# and that we are only seeing measurements for a few times per year at most\n",
    "\n",
    "cleaned_df_3['src_date'] = pd.to_datetime(cleaned_df_3['src_date'])\n",
    "cleaned_df_3['anlys_time'] = pd.to_datetime(cleaned_df_3['anlys_time'])\n",
    "cleaned_df_3['release_dt'] = pd.to_datetime(cleaned_df_3['release_dt'])\n",
    "\n",
    "\n",
    "Converted the times to unix. Unix times before 1/1/1970 are negative numbers, descending from 0 below 1/1/1970, so they'll still work and be\n",
    "# convertible and comparable.\n",
    "# Of note, minimum recording is 12/13/1901, so if there were dates before that, this would not be feasible.\n",
    "\n",
    "cleaned_df_3['src_date'] = cleaned_df_3['src_date'].apply(lambda x: int(x.timestamp()))\n",
    "cleaned_df_3['anlys_time'] = cleaned_df_3['anlys_time'].apply(lambda x: int(x.timestamp()))\n",
    "cleaned_df_3['release_dt'] = cleaned_df_3['release_dt'].apply(lambda x: int(x.timestamp()))\n",
    "\n",
    "# format for gone_date. the object wasn't recognized as a time like the other was, so I explicitly defined the format.\n",
    "gone_date_raw_format = '%Y-%m-%d'\n",
    "\n",
    "# Converting gone_date to datetime.\n",
    "cleaned_df_3['gone_date'] = pd.to_datetime(cleaned_df_3['gone_date'], format=gone_date_raw_format, errors='coerce')\n",
    "\n",
    "# Converting gone_date to unix. If the glacier still exists, it's a NaT, so replacing it with e for exists.\n",
    "nat_replace_if_exists = 'e'\n",
    "cleaned_df_3['gone_date'] = cleaned_df_3['gone_date'].apply(\n",
    "    lambda x: int(x.timestamp()) if pd.notna(x) else nat_replace_if_exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef7f78-19d0-44bc-818a-502b20a2fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying data frame\n",
    "\n",
    "cleaned_df_4 = cleaned_df_3.copy()\n",
    "\n",
    "# Converting columns to float. errors='coerce' won't effect 0 values, but any non-number will be converted to nan. Will print between this conversion and\n",
    "# the formatting just to make sure.\n",
    "\n",
    "cleaned_df_4['area'] = pd.to_numeric(cleaned_df_4['area'], errors='coerce')\n",
    "cleaned_df_4['db_area'] = pd.to_numeric(cleaned_df_4['db_area'], errors='coerce')\n",
    "\n",
    "nat_replace = 'x'\n",
    "\n",
    "# nan_rows # no nan rows for either area or db_area, so we're good, per the below check.\n",
    "# nan_rows = cleaned_df_4[cleaned_df_4['db_area'].isna()]\n",
    "\n",
    "# converting to float with the trailing digits removed. The number is the number of the significant digits we round to.\n",
    "\n",
    "for column in ['area', 'db_area']:\n",
    "    cleaned_df_4[column] = cleaned_df_4[column].apply(lambda x: '{:.15g}'.format(x) if pd.notnull(x) else nat_replace)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c2ba6-3dd8-4173-9e77-a70aa36375d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the analysis ID column for cleaned_df_5. We LIKELY don't need it.\n",
    "\n",
    "cleaned_df_5 = cleaned_df_4.drop(columns=['anlys_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d31ea-8e4a-4e7a-856f-75735f05218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is to view how many unique glac_id exist per glac_name.\n",
    "\n",
    "# cleaned_df_6 = cleaned_df_5.copy()\n",
    "\n",
    "# # Group by glac_name and count unique glac_id values\n",
    "# glac_id_counts = cleaned_df_6.groupby('glac_name')['glac_id'].nunique().reset_index()\n",
    "\n",
    "# # Rename columns for clarity\n",
    "# glac_id_counts.columns = ['glac_name', 'unique_glac_id_count']\n",
    "\n",
    "# glac_id_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e2f53-3e67-434c-b5de-842ebcb9d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_value = 'Yalik Glacier'\n",
    "\n",
    "filtered_df_1 = cleaned_df_5[cleaned_df_5['glac_name'] == filter_value]\n",
    "\n",
    "filtered_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965cdb8-21f2-4f92-959f-9b1818b52986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still have the same number of rows. Check the cell above that lists row count for cleaned_df\n",
    "row_count = cleaned_df_5.shape[0]\n",
    "print(f\"Number of rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e83cb7-c238-4894-bebb-50f7d400c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_df.to_csv('../Resources/cleaned_Canada_Glaciers.csv', index=False) #342 megs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca9bec-8623-4958-8a71-5c5734da0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_df_2.to_csv('../Resources/cleaned_df_2.csv', index=False) #313 megs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dde813-751a-440a-833c-a0b7811a03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_df_3.to_csv('../Resources/cleaned_df_3.csv', index=False) #310 megs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4fdb7-5b61-4cf7-8669-acba9acaa092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_df_4.to_csv('../Resources/cleaned_df_4.csv', index=False) #309 megs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7759249-6220-404e-b5b4-7c4be0218658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_df_5.to_csv('../Resources/cleaned_df_5.csv', index=False) #309 megs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b82f9-d575-4817-837b-5a81151ae2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_6 = cleaned_df_5.copy()\n",
    "\n",
    "cleaned_df_6['first_coordinate'] = cleaned_df_6['Coordinates'].apply(lambda x: x.split(',0')[0])\n",
    "\n",
    "cleaned_df_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e63f40-9823-4b16-bfb3-f00d12925ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_6.drop(columns=['Coordinates'], inplace=True)\n",
    "\n",
    "cleaned_df_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feda7ffc-3fad-472f-8b78-8319e9fe158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_7 = cleaned_df_6.copy()\n",
    "\n",
    "cleaned_df_7[['lng', 'lat']] = cleaned_df_7['first_coordinate'].str.split(',', expand=True)\n",
    "\n",
    "cleaned_df_7['lat'] = pd.to_numeric(cleaned_df_7['lat'])\n",
    "cleaned_df_7['lng'] = pd.to_numeric(cleaned_df_7['lng'])\n",
    "\n",
    "cleaned_df_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07006b59-83d3-4f9c-9cc9-94f9547ddca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_df_7.drop(columns=['first_coordinate'], inplace=True)\n",
    "\n",
    "avg_lat_lng_df = cleaned_df_7.groupby('glac_name')[['lat', 'lng']].mean().reset_index()\n",
    "\n",
    "avg_lat_lng_df\n",
    "\n",
    "avg_lat_lng_df.to_csv('../Resources/glacier_avg_lat_lng.csv', index=False)\n",
    "\n",
    "cleaned_df_7.to_csv('../Resources/cleaned_df_7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88a09f-edfc-4a16-923f-8a9d9fd4493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of clusters\n",
    "# num_clusters = 20\n",
    "\n",
    "# # Initialize KMeans with the number of clusters\n",
    "# kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "\n",
    "# # Fit the KMeans model\n",
    "# cleaned_df_7[['lat', 'lng']] = cleaned_df_7[['lat', 'lng']].astype(float)\n",
    "# cleaned_df_7['cluster'] = kmeans.fit_predict(cleaned_df_7[['lat', 'lng']])\n",
    "\n",
    "# # Calculate the average coordinates for each cluster\n",
    "# cluster_means = cleaned_df_7.groupby('cluster')[['lat', 'lng']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b7e4c5-8bb3-4c24-8628-dd6de27a1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cluster_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748353f-1411-4a01-ac5a-6946d61608fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters\n",
    "num_clusters = 100\n",
    "\n",
    "# Initialize KMeans with the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "\n",
    "# Fit the KMeans model\n",
    "cleaned_df_7[['lat', 'lng']] = cleaned_df_7[['lat', 'lng']].astype(float)\n",
    "cleaned_df_7['cluster'] = kmeans.fit_predict(cleaned_df_7[['lat', 'lng']])\n",
    "\n",
    "# Calculate the average coordinates for each cluster\n",
    "cluster_means = cleaned_df_7.groupby('cluster')[['lat', 'lng']].mean().reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot cluster centers\n",
    "plt.scatter(cluster_means['lng'], cluster_means['lat'], c='red', s=200, marker='X', label='Centroids')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Cluster Centers of Mean Latitude and Longitude')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "cluster_means.to_csv('../Resources/100_cluster_means.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387564e-fd9a-4678-bf4b-a7977daecf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cluster means to DataFrame\n",
    "cluster_means_df = pd.DataFrame(cluster_means, columns=['lat', 'lng'])\n",
    "\n",
    "# Create a column 'cluster_id' for index purposes starting at 1\n",
    "cluster_means_df['cluster_id'] = range(1, len(cluster_means_df) + 1)\n",
    "\n",
    "# Set 'cluster_id' as the index\n",
    "cluster_means_df.set_index('cluster_id', inplace=True)\n",
    "\n",
    "kdtree = KDTree(cluster_means_df[['lat', 'lng']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ef95f-9c26-4a3c-9d23-8c819f5c3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_coords = cleaned_df_7[['lat', 'lng']].values\n",
    "\n",
    "distances, indices = kdtree.query(data_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6728e-bd80-4b11-9a00-25048c39eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_7['closest_cluster_lat'] = cluster_means_df['lat'].iloc[indices].values\n",
    "cleaned_df_7['closest_cluster_lng'] = cluster_means_df['lng'].iloc[indices].values\n",
    "\n",
    "cleaned_df_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1865b-d210-459d-a418-ab940a5637df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "filtered_df = cleaned_df_7[cleaned_df_7['area'] != '0']\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fae05c-43fd-4f7f-9fe1-ac617b21afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "filtered_df_carrie = cleaned_df_7[cleaned_df_7['glac_name'] == 'Carrie Glacier']\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "filtered_df_carrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8900f-bc29-40c8-aa91-7899b734b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_8 = cleaned_df_7.copy()\n",
    "\n",
    "# cleaned_df_8 = cleaned_df_8[cleaned_df_8['area'] != '0']\n",
    "cleaned_df_8 = cleaned_df_8[cleaned_df_8['glac_name'] != 'None']\n",
    "\n",
    "cleaned_df_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53eabb-e5fb-42fb-808a-1ad5de6f2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = cleaned_df_8['glac_name'].value_counts()\n",
    "\n",
    "cleaned_df_9 = cleaned_df_8[cleaned_df_8['glac_name'].isin(counts[counts >= 2].index)]\n",
    "\n",
    "cleaned_df_9\n",
    "\n",
    "cleaned_df_9.to_csv('../Resources/us_glaciers_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b956b23-97a7-4e25-8815-69a3170811ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_10 = cleaned_df_9.copy()\n",
    "\n",
    "# Convert 'area' to numeric type\n",
    "cleaned_df_10['area'] = pd.to_numeric(cleaned_df_10['area'])\n",
    "\n",
    "# Sort by glacier name and time (Unix timestamps)\n",
    "cleaned_df_11 = cleaned_df_10.sort_values(by=['glac_name', 'anlys_time'])\n",
    "\n",
    "# Keep only the first occurrence for each timestamp within each glacier\n",
    "cleaned_df_11 = cleaned_df_11.drop_duplicates(subset=['glac_name', 'anlys_time'], keep='first')\n",
    "\n",
    "# Find the initial area value for each glac_name\n",
    "initial_area = cleaned_df_11.groupby('glac_name')['area'].transform('first')\n",
    "\n",
    "# Convert 'initial_area' to numeric type (if necessary)\n",
    "initial_area = pd.to_numeric(initial_area)\n",
    "\n",
    "# Set the initial value to 100\n",
    "cleaned_df_11['percent_area'] = 100 + (cleaned_df_11['area'] - initial_area) / initial_area * 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3e690-2459-415d-824c-af9bcf27ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate counts for each glac_name\n",
    "counts = cleaned_df_11['glac_name'].value_counts()\n",
    "\n",
    "# Identify glac_name values that occur more than once\n",
    "valid_glac_names = counts[counts > 1].index\n",
    "\n",
    "# Filter DataFrame to keep only rows with glac_name in valid_glac_names\n",
    "cleaned_df_11 = cleaned_df_11[cleaned_df_11['glac_name'].isin(valid_glac_names)]\n",
    "\n",
    "cleaned_df_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcd8e4-a74a-489a-a7c9-96ebefab385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_zero_area = cleaned_df_11[cleaned_df_11['area'] == 0]\n",
    "\n",
    "rows_with_zero_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d251e-accf-453a-b4ab-2772d964b3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
